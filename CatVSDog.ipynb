{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, PILToTensor, Resize, Normalize\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "cat_folder = './PetImages/Cat'\n",
    "dog_folder = './PetImages/Dog'\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((256, 256)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5,), std=(0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, folder, label, transform=None):\n",
    "        self.folder = folder\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.images = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith('.jpg') or file.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "# dataset\n",
    "cat_dataset = PetDataset(cat_folder, label=0, transform=transform)\n",
    "dog_dataset = PetDataset(dog_folder, label=1, transform=transform)\n",
    "\n",
    "# dataloader\n",
    "full_dataset = cat_dataset + dog_dataset\n",
    "train_loader = DataLoader(full_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # Input: (3, 256, 256), Output: (32, 256, 256)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)  # Output: (64, 128, 128)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # Output: (128, 64, 64)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Halves the dimensions\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1_input_size = 128 * (256 // 2 // 2 // 2) * (256 // 2 // 2 // 2)\n",
    "        self.fc1 = nn.Linear(self.fc1_input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)  # Output: (32, 128, 128)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # Output: (64, 64, 64)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)  # Output: (128, 32, 32)\n",
    "\n",
    "        x = self.flatten(x)  # Flatten to (batch_size, 128 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\Lib\\site-packages\\PIL\\TiffImagePlugin.py:870: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6009597917015735\n",
      "Epoch 2, Loss: 0.4453740326448893\n",
      "Epoch 3, Loss: 0.326827213798578\n",
      "Epoch 4, Loss: 0.1823820491369145\n",
      "Epoch 5, Loss: 0.07069133189715779\n",
      "Epoch 6, Loss: 0.03849659207309089\n",
      "Epoch 7, Loss: 0.02690229140614228\n",
      "Epoch 8, Loss: 0.02880248682992645\n",
      "Epoch 9, Loss: 0.017933116875755307\n",
      "Epoch 10, Loss: 0.02456670050332599\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 99.72%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(model, data_loader, device):\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
